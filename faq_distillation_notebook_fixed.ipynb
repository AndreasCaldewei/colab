{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreasCaldewei/colab/blob/main/faq_distillation_notebook_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# FAQ System with Model Distillation\n",
        "\n",
        "This notebook demonstrates how to build an FAQ system using model distillation. We'll transfer knowledge from a large language model to a smaller, more efficient model that can quickly answer frequently asked questions.\n",
        "\n",
        "## What is this project?\n",
        "\n",
        "- **Goal**: Create an FAQ system that can quickly match user questions to the most relevant answers\n",
        "- **Approach**: Use model distillation to create efficient semantic matching\n",
        "- **Benefits**: Fast response times, works offline, and requires minimal resources\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Step 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packages",
        "outputId": "7716b4cb-ab81-4be0-d839-6e43d9ff6a11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers datasets tqdm joblib torch scikit-learn matplotlib pandas numpy"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/363.4 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import-libraries"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import joblib\n",
        "import re\n",
        "import json"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "check-device"
      },
      "source": [
        "## Step 3: Check for GPU and Configure Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# If using GPU, check which one we have\n",
        "if device.type == \"cuda\":\n",
        "    !nvidia-smi\n",
        "\n",
        "# Configuration settings\n",
        "max_length = 128  # Maximum sequence length for the model\n",
        "batch_size = 8    # Batch size for processing\n",
        "seed = 42         # Random seed for reproducibility\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teacher-model"
      },
      "source": [
        "## Step 4: Load the Teacher Model\n",
        "\n",
        "We'll use BERT or another transformer model as our teacher model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model"
      },
      "source": [
        "print(\"Loading the teacher model...\")\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Optimized for sentence similarity\n",
        "# Alternative options:\n",
        "# model_name = \"bert-base-uncased\"  # Standard BERT\n",
        "# model_name = \"distilbert-base-uncased\"  # Smaller, faster model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "teacher_model = AutoModel.from_pretrained(model_name)\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()  # Set to evaluation mode\n",
        "print(f\"Loaded {model_name} model\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faq-dataset"
      },
      "source": [
        "## Step 5: Create and Prepare FAQ Dataset\n",
        "\n",
        "Let's define our FAQ dataset. You can replace this with your own custom FAQ data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-dataset"
      },
      "source": [
        "# Define a sample FAQ dataset (replace with your own data)\n",
        "sample_faqs = [\n",
        "    {\n",
        "        \"question\": \"What is model distillation?\",\n",
        "        \"answer\": \"Model distillation is a technique for transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) while maintaining as much performance as possible.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does model distillation work?\",\n",
        "        \"answer\": \"Model distillation works by training a smaller model to mimic the behavior or outputs of a larger model, often by using the larger model's predictions or embeddings as training signals.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the benefits of using distilled models?\",\n",
        "        \"answer\": \"Distilled models are smaller, faster, and require less computational resources while retaining most of the performance of larger models. They're ideal for deployment in resource-constrained environments.\",\n",
        "        \"category\": \"Benefits\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can I use distilled models on mobile devices?\",\n",
        "        \"answer\": \"Yes, distilled models are well-suited for mobile applications due to their smaller size and faster inference times.\",\n",
        "        \"category\": \"Applications\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What's the difference between model distillation and model pruning?\",\n",
        "        \"answer\": \"Model distillation trains a new, smaller model using a larger model's outputs, while pruning reduces the size of an existing model by removing unnecessary parameters.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How much smaller can a distilled model be?\",\n",
        "        \"answer\": \"Distilled models can be significantly smaller, sometimes 10-50 times smaller than the original teacher model, depending on the architecture and distillation approach.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Are distilled models as accurate as the original models?\",\n",
        "        \"answer\": \"Distilled models typically retain 90-95% of the performance of the original teacher model, with the exact percentage depending on the task complexity and model architectures.\",\n",
        "        \"category\": \"Performance\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What type of tasks can I use distilled models for?\",\n",
        "        \"answer\": \"Distilled models work well for a wide range of tasks including classification, retrieval, question answering, and text embedding generation. They're particularly effective for well-defined tasks with clear outputs.\",\n",
        "        \"category\": \"Applications\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How do I choose the right size for my student model?\",\n",
        "        \"answer\": \"The optimal student model size depends on your resource constraints and performance requirements. Start with a model 3-10x smaller than the teacher and evaluate the performance/size tradeoff.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can I distill any type of model?\",\n",
        "        \"answer\": \"Most model types can be distilled, including neural networks, transformer models, and decision trees. The distillation process varies by model type but the core principle of knowledge transfer remains the same.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are some popular model distillation techniques?\",\n",
        "        \"answer\": \"Popular techniques include soft target distillation (using probability distributions), feature distillation (matching intermediate representations), and data augmentation during distillation to improve generalization.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How long does the distillation process take?\",\n",
        "        \"answer\": \"Distillation typically takes less time than training the original teacher model from scratch. Depending on the model size and data volume, it can range from hours to days on standard hardware.\",\n",
        "        \"category\": \"Process\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is temperature in knowledge distillation?\",\n",
        "        \"answer\": \"Temperature is a hyperparameter in distillation that controls the softness of probability distributions from the teacher model. Higher temperatures produce softer distributions that better transfer knowledge about relationships between classes.\",\n",
        "        \"category\": \"Technical\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can I create an ensemble of distilled models?\",\n",
        "        \"answer\": \"Yes, ensembles of multiple distilled models can be effective, offering improved performance while still being more efficient than the original large model.\",\n",
        "        \"category\": \"Applications\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What's the history of model distillation?\",\n",
        "        \"answer\": \"Model distillation was popularized by Geoffrey Hinton in 2015 with his paper 'Distilling the Knowledge in a Neural Network,' though similar concepts existed earlier under different names.\",\n",
        "        \"category\": \"Background\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Extract questions, answers, and categories from the sample FAQs\n",
        "questions = [faq[\"question\"] for faq in sample_faqs]\n",
        "answers = [faq[\"answer\"] for faq in sample_faqs]\n",
        "categories = [faq[\"category\"] for faq in sample_faqs]\n",
        "\n",
        "# Create additional variations of each question for better training\n",
        "question_variations = []\n",
        "answer_variations = []\n",
        "category_variations = []\n",
        "\n",
        "for i, q in enumerate(questions):\n",
        "    # Original question\n",
        "    question_variations.append(q)\n",
        "    answer_variations.append(answers[i])\n",
        "    category_variations.append(categories[i])\n",
        "\n",
        "    # Add variations - remove question marks, rephrase slightly\n",
        "    variations = [\n",
        "        q.replace(\"?\", \"\"),\n",
        "        \"Can you tell me \" + q.lower(),\n",
        "        \"I'd like to know \" + q.lower(),\n",
        "        \"Tell me about \" + q.lower().replace(\"what is \", \"\").replace(\"how does \", \"\").replace(\"?\", \"\")\n",
        "    ]\n",
        "\n",
        "    for var in variations:\n",
        "        question_variations.append(var)\n",
        "        answer_variations.append(answers[i])\n",
        "        category_variations.append(categories[i])\n",
        "\n",
        "print(f\"Original FAQs: {len(questions)}\")\n",
        "print(f\"With variations: {len(question_variations)}\")\n",
        "\n",
        "# Display a few examples of the variations\n",
        "for i in range(3):\n",
        "    print(f\"\\nOriginal: {questions[i]}\")\n",
        "    idx = i * 5  # Each original question has 4 variations + original\n",
        "    for j in range(1, 5):\n",
        "        print(f\"Variation {j}: {question_variations[idx + j]}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "embedding-function"
      },
      "source": [
        "## Step 6: Define Embedding Extraction Function\n",
        "\n",
        "This function will extract embeddings from our teacher model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract-embeddings"
      },
      "source": [
        "def get_model_embeddings(texts, tokenizer, model, batch_size=8, max_length=128):\n",
        "    \"\"\"Extract embeddings from the teacher model\n",
        "\n",
        "    Args:\n",
        "        texts (list): List of text strings to embed\n",
        "        tokenizer: The tokenizer for the model\n",
        "        model: The model to use for embedding\n",
        "        batch_size (int): Number of texts to process at once\n",
        "        max_length (int): Maximum sequence length for tokenization\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of embeddings, shape (len(texts), embedding_dim)\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    # Process in batches\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "        with torch.no_grad():  # No need to track gradients\n",
        "            # Tokenize the text\n",
        "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                             truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get the hidden states from the model\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            # Use the last hidden state as embeddings\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "            # Use mean pooling to get a fixed-size vector representation\n",
        "            mean_embeddings = last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "            for emb in mean_embeddings:\n",
        "                embeddings.append(emb)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Function to normalize embeddings (improves performance for cosine similarity)\n",
        "def normalize_embeddings(embeddings):\n",
        "    \"\"\"Normalize embeddings to unit length\"\"\"\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    return embeddings / norms"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extract-teacher-embeddings"
      },
      "source": [
        "## Step 7: Extract Embeddings from the Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process-teacher"
      },
      "source": [
        "print(\"Extracting embeddings for FAQ questions...\")\n",
        "start_time = time.time()\n",
        "question_embeddings = get_model_embeddings(question_variations, tokenizer, teacher_model, batch_size=batch_size)\n",
        "extraction_time = time.time() - start_time\n",
        "\n",
        "print(f\"Embedding extraction took {extraction_time:.2f} seconds\")\n",
        "print(f\"Embedding shape: {question_embeddings.shape}\")\n",
        "\n",
        "# Normalize embeddings for better similarity matching\n",
        "question_embeddings = normalize_embeddings(question_embeddings)\n",
        "\n",
        "# Visualize a few embeddings\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(question_embeddings[:10, :50], aspect='auto', cmap='viridis')\n",
        "plt.colorbar()\n",
        "plt.title('First 10 question embeddings (first 50 dimensions)')\n",
        "plt.xlabel('Embedding dimension')\n",
        "plt.ylabel('Question')\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "student-models"
      },
      "source": [
        "## Step 8: Train Student Models\n",
        "\n",
        "We'll train multiple student models for our FAQ system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-student-models"
      },
      "source": [
        "# Prepare data for training\n",
        "# For simplicity, we'll convert category strings to numerical labels\n",
        "unique_categories = list(set(category_variations))\n",
        "category_to_id = {cat: idx for idx, cat in enumerate(unique_categories)}\n",
        "category_ids = [category_to_id[cat] for cat in category_variations]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    question_embeddings,\n",
        "    category_ids,\n",
        "    test_size=0.2,\n",
        "    random_state=seed,\n",
        "    stratify=category_ids  # Ensure same category distribution in train/test\n",
        ")\n",
        "\n",
        "# Initialize student models\n",
        "print(\"Training student models...\")\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=seed),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=seed)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "train_times = {}\n",
        "\n",
        "# Train each model and measure performance\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = accuracy\n",
        "    train_times[name] = train_time\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy:.4f}, Training time: {train_time:.2f} seconds\")\n",
        "    print(\"  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=unique_categories))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## Step 9: Visualize Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize-models"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(results.keys(), results.values(), color=['#3498db', '#2ecc71'])\n",
        "plt.title('Model Accuracy', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for i, (key, value) in enumerate(results.items()):\n",
        "    plt.text(i, value + 0.02, f'{value:.4f}', ha='center', fontsize=11)\n",
        "\n",
        "# Training time comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(train_times.keys(), train_times.values(), color=['#3498db', '#2ecc71'])\n",
        "plt.title('Training Time', fontsize=14)\n",
        "plt.ylabel('Time (seconds)', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for i, (key, value) in enumerate(train_times.items()):\n",
        "    plt.text(i, value + 0.5, f'{value:.2f}s', ha='center', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = max(results, key=results.get)\n",
        "print(f\"The best performing model is: {best_model_name} with accuracy: {results[best_model_name]:.4f}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nearest-neighbors"
      },
      "source": [
        "## Step 10: Create a Nearest Neighbors Model for FAQ Matching\n",
        "\n",
        "We'll use a nearest neighbors approach to find the most similar questions in our database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-nn"
      },
      "source": [
        "# Extract original question embeddings (without variations)\n",
        "original_questions = questions  # Keep a copy of original questions\n",
        "original_answers = answers      # Keep a copy of original answers\n",
        "\n",
        "print(\"Creating embeddings for the original questions...\")\n",
        "original_embeddings = get_model_embeddings(original_questions, tokenizer, teacher_model)\n",
        "original_embeddings = normalize_embeddings(original_embeddings)\n",
        "\n",
        "# Create a nearest neighbors model\n",
        "print(\"Training nearest neighbors model...\")\n",
        "nn_model = NearestNeighbors(n_neighbors=3, metric='cosine')\n",
        "nn_model.fit(original_embeddings)\n",
        "\n",
        "# Function to find the most similar question and return its answer\n",
        "def get_answer(query, nn_model, tokenizer, teacher_model, questions, answers, threshold=0.2):\n",
        "    \"\"\"Get the most relevant answer for a given query\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question\n",
        "        nn_model: Trained nearest neighbors model\n",
        "        tokenizer: The tokenizer for the embedding model\n",
        "        teacher_model: The model to use for embedding\n",
        "        questions (list): List of original questions\n",
        "        answers (list): List of original answers\n",
        "        threshold (float): Maximum distance threshold for a match\n",
        "\n",
        "    Returns:\n",
        "        tuple: (matched_question, answer, distance, confidence)\n",
        "    \"\"\"\n",
        "    # Get query embedding\n",
        "    query_embedding = get_model_embeddings([query], tokenizer, teacher_model)\n",
        "    query_embedding = normalize_embeddings(query_embedding)\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    distances, indices = nn_model.kneighbors(query_embedding)\n",
        "\n",
        "    # Get closest match\n",
        "    closest_idx = indices[0][0]\n",
        "    distance = distances[0][0]\n",
        "\n",
        "    # Calculate confidence (1 - distance)\n",
        "    confidence = 1 - distance\n",
        "\n",
        "    # Check if match is good enough\n",
        "    if distance > threshold:\n",
        "        return (None, \"I don't have information on that specific question.\", distance, confidence)\n",
        "\n",
        "    return (questions[closest_idx], answers[closest_idx], distance, confidence)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faq-system"
      },
      "source": [
        "## Step 11: Create a Complete FAQ System Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faq-class"
      },
      "source": [
        "class FAQSystem:\n",
        "    \"\"\"A distilled model-based FAQ system\"\"\"\n",
        "\n",
        "    def __init__(self, teacher_model, tokenizer, nn_model, category_model,\n",
        "                 questions, answers, categories, category_mapping):\n",
        "        self.teacher_model = teacher_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.nn_model = nn_model\n",
        "        self.category_model = category_model\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.categories = categories\n",
        "        self.category_mapping = category_mapping\n",
        "        self.id_to_category = {v: k for k, v in category_mapping.items()}\n",
        "\n",
        "    def get_answer(self, query, threshold=0.2):\n",
        "        \"\"\"Get answer for a user question\"\"\"\n",
        "        return get_answer(query, self.nn_model, self.tokenizer, self.teacher_model,\n",
        "                          self.questions, self.answers, threshold)\n",
        "\n",
        "    def predict_category(self, query):\n",
        "        \"\"\"Predict the category of a question\"\"\"\n",
        "        # Get query embedding\n",
        "        query_embedding = get_model_embeddings([query], self.tokenizer, self.teacher_model)\n",
        "\n",
        "        # Predict category\n",
        "        category_id = self.category_model.predict(query_embedding)[0]\n",
        "        category = self.id_to_category[category_id]\n",
        "\n",
        "        return category\n",
        "\n",
        "    def get_questions_by_category(self, category):\n",
        "        \"\"\"Get all questions in a specific category\"\"\"\n",
        "        result = []\n",
        "        for q, c in zip(self.questions, self.categories):\n",
        "                result.append(q)\n",
        "        return result\n",
        "\n",
        "    def save(self, filename=\"faq_system.joblib\"):\n",
        "        \"\"\"Save the FAQ system to disk\"\"\"\n",
        "        # Create a dictionary with everything we need to restore the system\n",
        "        system_data = {\n",
        "            \"questions\": self.questions,\n",
        "            \"answers\": self.answers,\n",
        "            \"categories\": self.categories,\n",
        "            \"category_mapping\": self.category_mapping,\n",
        "            \"nn_model\": self.nn_model,\n",
        "            \"category_model\": self.category_model,\n",
        "            \"model_name\": self.tokenizer.name_or_path\n",
        "        }\n",
        "\n",
        "        joblib.dump(system_data, filename)\n",
        "        print(f\"FAQ system saved to {filename}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename=\"faq_system.joblib\"):\n",
        "        \"\"\"Load a saved FAQ system\"\"\"\n",
        "        system_data = joblib.load(filename)\n",
        "\n",
        "        # Load the model and tokenizer\n",
        "        model_name = system_data[\"model_name\"]\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        teacher_model = AutoModel.from_pretrained(model_name)\n",
        "        teacher_model.eval()\n",
        "\n",
        "        # Create instance\n",
        "        return cls(\n",
        "            teacher_model=teacher_model,\n",
        "            tokenizer=tokenizer,\n",
        "            nn_model=system_data[\"nn_model\"],\n",
        "            category_model=system_data[\"category_model\"],\n",
        "            questions=system_data[\"questions\"],\n",
        "            answers=system_data[\"answers\"],\n",
        "            categories=system_data[\"categories\"],\n",
        "            category_mapping=system_data[\"category_mapping\"]\n",
        "        )"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create-faq-system"
      },
      "source": [
        "## Step 12: Create and Save the FAQ System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "build-faq-system"
      },
      "source": [
        "# Create the full FAQ system\n",
        "print(\"Building the complete FAQ system...\")\n",
        "\n",
        "# Use the best performing model for category prediction\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "# Create our FAQ system\n",
        "faq_system = FAQSystem(\n",
        "    teacher_model=teacher_model,\n",
        "    tokenizer=tokenizer,\n",
        "    nn_model=nn_model,\n",
        "    category_model=best_model,\n",
        "    questions=original_questions,\n",
        "    answers=original_answers,\n",
        "    categories=categories[:len(original_questions)],  # Only use original categories\n",
        "    category_mapping=category_to_id\n",
        ")\n",
        "\n",
        "# Save the FAQ system\n",
        "faq_system.save(\"distilled_faq_system.joblib\")\n",
        "\n",
        "# Download the saved model (if running in Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('distilled_faq_system.joblib')\n",
        "    print(\"Model downloaded. You can use this file to load the FAQ system later.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab. Model saved locally.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-faq-system"
      },
      "source": [
        "## Step 13: Test the FAQ System with Sample Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-system"
      },
      "source": [
        "print(\"Testing the FAQ system with sample questions...\\n\")\n",
        "\n",
        "test_questions = [\n",
        "    \"What exactly is model distillation?\",\n",
        "    \"I want to know if distilled models can run on my phone\",\n",
        "    \"Are smaller models less accurate?\",\n",
        "    \"Tell me about the benefits of model distillation\",\n",
        "    \"What's the relationship between model distillation and knowledge transfer?\",\n",
        "    \"Why would I want to use a distilled model?\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(test_questions):\n",
        "    print(f\"\\nQuestion {i+1}: {question}\")\n",
        "\n",
        "    # Get answer\n",
        "    matched_question, answer, distance, confidence = faq_system.get_answer(question)\n",
        "\n",
        "    # Get category\n",
        "    category = faq_system.predict_category(question)\n",
        "\n",
        "    print(f\"Category: {category}\")\n",
        "    print(f\"Matched question: {matched_question}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive-demo"
      },
      "source": [
        "## Step 14: Interactive Demo\n",
        "\n",
        "Let's create an interactive demo to test our FAQ system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo"
      },
      "source": [
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Create input widget\n",
        "question_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your question here',\n",
        "    description='Question:',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "# Create output widget\n",
        "output = widgets.Output()\n",
        "\n",
        "# Create button\n",
        "button = widgets.Button(\n",
        "    description='Ask',\n",
        "    button_style='primary',\n",
        "    tooltip='Ask the FAQ system'\n",
        ")\n",
        "\n",
        "# Create confidence threshold slider\n",
        "threshold_slider = widgets.FloatSlider(\n",
        "    value=0.2,\n",
        "    min=0.0,\n",
        "    max=0.5,\n",
        "    step=0.05,\n",
        "    description='Threshold:',\n",
        "    tooltip='Confidence threshold for answers',\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "# Function to handle button click\n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        question = question_input.value\n",
        "        if not question:\n",
        "            print(\"Please enter a question.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Question: {question}\")\n",
        "\n",
        "        # Get answer with current threshold\n",
        "        matched_question, answer, distance, confidence = faq_system.get_answer(\n",
        "            question, threshold=threshold_slider.value\n",
        "        )\n",
        "\n",
        "        # Get category\n",
        "        category = faq_system.predict_category(question)\n",
        "\n",
        "        print(f\"\\nCategory: {category}\")\n",
        "\n",
        "        if matched_question:\n",
        "            print(f\"\\nMatched question: {matched_question}\")\n",
        "            print(f\"Confidence: {confidence:.2f}\")\n",
        "            print(f\"\\nAnswer: {answer}\")\n",
        "        else:\n",
        "            print(f\"\\nNo matching question found (confidence: {confidence:.2f})\")\n",
        "            print(\"Please rephrase your question or ask something related to model distillation.\")\n",
        "\n",
        "# Connect the button to the function\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(HTML(\"<h3>FAQ System Demo</h3>\"))\n",
        "display(HTML(\"<p>Ask any question about model distillation.</p>\"))\n",
        "display(widgets.HBox([question_input, button]))\n",
        "display(threshold_slider)\n",
        "display(output)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage-instructions"
      },
      "source": [
        "## Step 15: How to Use the Saved FAQ System\n",
        "\n",
        "Here's how to load and use your saved FAQ system in a production environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "production-usage"
      },
      "source": [
        "# This is for reference - how to load and use the saved FAQ system\n",
        "\n",
        "'''\n",
        "# Import required libraries\n",
        "import joblib\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the saved FAQ system\n",
        "loaded_faq_system = FAQSystem.load(\"distilled_faq_system.joblib\")\n",
        "\n",
        "# Use the system to answer questions\n",
        "question = \"What is model distillation?\"\n",
        "matched_question, answer, distance, confidence = loaded_faq_system.get_answer(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Confidence: {confidence:.2f}\")\n",
        "'''"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extend-instructions"
      },
      "source": [
        "## How to Extend This FAQ System\n",
        "\n",
        "To extend this FAQ system with your own data:\n",
        "\n",
        "1. Create a list of your own FAQs in the format used in this notebook\n",
        "2. Replace the `sample_faqs` variable with your own data\n",
        "3. Re-run the notebook to train the models on your data\n",
        "4. Save the resulting FAQ system for deployment\n",
        "\n",
        "You can also fine-tune various parameters like:\n",
        "- The confidence threshold for matching questions\n",
        "- The teacher model used for embedding\n",
        "- The student models used for classification\n",
        "\n",
        "For larger FAQ datasets, consider adding pagination to the results and implementing more sophisticated matching algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've built a complete FAQ system using model distillation. The system leverages a large pre-trained language model as a teacher to generate high-quality embeddings, then uses smaller, more efficient models as students for fast inference.\n",
        "\n",
        "The result is a powerful FAQ system that can:\n",
        "- Match user questions to the most relevant answers\n",
        "- Categorize questions by topic\n",
        "- Run efficiently with minimal resources\n",
        "- Be easily extended with your own FAQ data\n",
        "\n",
        "This approach demonstrates the power of model distillation for creating practical, deployable AI systems by transferring knowledge from large models to smaller, more efficient ones."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}