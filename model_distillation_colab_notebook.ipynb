{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreasCaldewei/colab/blob/main/model_distillation_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAjYUpiTxuKc"
      },
      "source": [
        "# Model Distillation in Google Colab\n",
        "\n",
        "This notebook demonstrates how to perform model distillation - transferring knowledge from a large pre-trained language model (teacher) to smaller, more efficient models (students) using scikit-learn.\n",
        "\n",
        "## What is Model Distillation?\n",
        "\n",
        "Model distillation is a technique where we:\n",
        "1. Take a large, powerful model (teacher)\n",
        "2. Extract the knowledge/understanding from that model\n",
        "3. Transfer that knowledge to a smaller, faster model (student)\n",
        "\n",
        "In this notebook, we'll use a pre-trained language model as our teacher and train various scikit-learn models as our students."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgolCzFixuKd"
      },
      "source": [
        "## Step 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DMh_cGlxuKd"
      },
      "source": [
        "!pip install transformers datasets tqdm joblib torch scikit-learn matplotlib"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtBu2h1IxuKd"
      },
      "source": [
        "## Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow9OwNTbxuKd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import joblib"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ3euQELxuKe"
      },
      "source": [
        "## Step 3: Setup and Configuration\n",
        "\n",
        "Let's configure our environment and check if we have GPU access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlIN0wa9xuKe"
      },
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# If using GPU, check which one we have\n",
        "if device.type == \"cuda\":\n",
        "    !nvidia-smi\n",
        "\n",
        "# Configuration settings\n",
        "max_length = 128  # Maximum sequence length for the model\n",
        "num_samples = 1000  # Number of examples to use (increase if you have more memory)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGy0uVrUxuKe"
      },
      "source": [
        "## Step 4: Load the Teacher Model\n",
        "\n",
        "We'll use BERT as our teacher model. This is openly available and runs efficiently on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1Zh7i-cxuKe"
      },
      "source": [
        "print(\"Loading the teacher model...\")\n",
        "model_name = \"bert-base-uncased\"  # Alternative: \"distilbert-base-uncased\" for faster processing\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "teacher_model = AutoModel.from_pretrained(model_name)\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()  # Set to evaluation mode\n",
        "print(f\"Loaded {model_name} model\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUWD3FXcxuKe"
      },
      "source": [
        "## Optional: Authenticate with Hugging Face to Access Llama\n",
        "\n",
        "If you have access to Llama models from Hugging Face, uncomment and run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQG6d4tkxuKe"
      },
      "source": [
        "# from huggingface_hub import login\n",
        "# login(\"your_huggingface_token_here\")\n",
        "# model_name = \"meta-llama/Llama-2-7b-hf\"  # Or another Llama model you have access to\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# teacher_model = AutoModel.from_pretrained(model_name)\n",
        "# teacher_model = teacher_model.to(device)\n",
        "# teacher_model.eval()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVCDnpC8xuKe"
      },
      "source": [
        "## Step 5: Prepare the Dataset\n",
        "\n",
        "We'll use the IMDB dataset for sentiment analysis."
      ]
    },
    {
      "source": [
        "print(\"Preparing the dataset...\")\n",
        "# Load a subset of the IMDB dataset for sentiment analysis\n",
        "dataset = load_dataset(\"imdb\", split=f\"train[:{num_samples}]\")\n",
        "\n",
        "# Extract texts and labels\n",
        "texts = dataset[\"text\"]\n",
        "labels = dataset[\"label\"]\n",
        "\n",
        "print(f\"Loaded {len(texts)} examples from IMDB dataset\")\n",
        "print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "# Show a couple of examples\n",
        "print(\"\\nExample positive review:\")\n",
        "# Find the index of the first positive review, or use 0 if not found\n",
        "pos_idx = next((i for i, x in enumerate(labels) if x == 1), 0)\n",
        "print(f\"{texts[pos_idx][:300]}...\")\n",
        "\n",
        "print(\"\\nExample negative review:\")\n",
        "# Find the index of the first negative review, or use 0 if not found\n",
        "neg_idx = next((i for i, x in enumerate(labels) if x == 0), 0)\n",
        "print(f\"{texts[neg_idx][:300]}...\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Vj84IQrxyzQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmWEv72SxuKe"
      },
      "source": [
        "## Step 6: Define Function to Extract Embeddings from Teacher Model\n",
        "\n",
        "This function extracts the knowledge from our teacher model in the form of embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpsv6lMIxuKe"
      },
      "source": [
        "def get_model_embeddings(texts, tokenizer, model):\n",
        "    \"\"\"Extract embeddings from the teacher model\"\"\"\n",
        "    embeddings = []\n",
        "    batch_size = 8  # Adjust based on your GPU memory\n",
        "\n",
        "    # Process in batches to speed things up\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "        with torch.no_grad():  # No need to track gradients\n",
        "            # Tokenize the text\n",
        "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                             truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get the hidden states from the model\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            # Use the last hidden state as embeddings\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "            # Use mean pooling to get a fixed-size vector representation\n",
        "            # Shape: [batch_size, hidden_size]\n",
        "            mean_embeddings = last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "            for emb in mean_embeddings:\n",
        "                embeddings.append(emb)\n",
        "\n",
        "    return np.array(embeddings)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I_SRYbHxuKe"
      },
      "source": [
        "## Step 7: Extract Embeddings from the Teacher Model\n",
        "\n",
        "Now we'll use our function to extract embeddings for all examples in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnydGCYuxuKe"
      },
      "source": [
        "print(\"Extracting embeddings from the model...\")\n",
        "start_time = time.time()\n",
        "embeddings = get_model_embeddings(texts, tokenizer, teacher_model)\n",
        "extraction_time = time.time() - start_time\n",
        "\n",
        "print(f\"Embedding extraction took {extraction_time:.2f} seconds\")\n",
        "print(f\"Embedding shape: {embeddings.shape}\")\n",
        "\n",
        "# Quick visualization of the first few embeddings\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(embeddings[:10, :50], aspect='auto', cmap='viridis')\n",
        "plt.colorbar()\n",
        "plt.title('First 10 embeddings (first 50 dimensions)')\n",
        "plt.xlabel('Embedding dimension')\n",
        "plt.ylabel('Example')\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XN1vZcAxuKe"
      },
      "source": [
        "## Step 8: Split Data into Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdSretfpxuKe"
      },
      "source": [
        "# Fix 1: Load a balanced dataset with both classes\n",
        "print(\"Preparing the dataset...\")\n",
        "# Load equal numbers of positive and negative reviews\n",
        "pos_examples = load_dataset(\"imdb\", split=f\"train[0:{num_samples//2}]\")\n",
        "neg_examples = load_dataset(\"imdb\", split=f\"train[12500:{12500+num_samples//2}]\")\n",
        "\n",
        "# Combine them\n",
        "texts = pos_examples[\"text\"] + neg_examples[\"text\"]\n",
        "labels = np.array(pos_examples[\"label\"] + neg_examples[\"label\"])\n",
        "\n",
        "print(f\"Loaded {len(texts)} examples from IMDB dataset\")\n",
        "print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "# Fix 2: Use stratified train/test split to maintain class balance\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    embeddings, labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels  # This ensures the class distribution is preserved in both sets\n",
        ")\n",
        "\n",
        "# Convert labels to numpy arrays if they aren't already\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Verify we have both classes in the training data\n",
        "print(\"Training set label distribution:\", np.bincount(y_train))\n",
        "print(\"Test set label distribution:\", np.bincount(y_test))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF4CdwGHxuKe"
      },
      "source": [
        "## Step 9: Train Multiple Student Models\n",
        "\n",
        "Now we'll train several scikit-learn models as our students."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb6LvY-bxuKe"
      },
      "source": [
        "print(\"Training student models...\")\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"MLP (Neural Network)\": MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=1000, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "train_times = {}\n",
        "all_predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    all_predictions[name] = y_pred\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = accuracy\n",
        "    train_times[name] = train_time\n",
        "    print(f\"  Accuracy: {accuracy:.4f}, Training time: {train_time:.2f} seconds\")\n",
        "    print(\"  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuYeBah0xuKf"
      },
      "source": [
        "## Step 10: Visualize Results\n",
        "\n",
        "Let's create some visualizations to compare our student models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd4lozARxuKf"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(results.keys(), results.values(), color=['#3498db', '#2ecc71', '#e74c3c'])\n",
        "plt.title('Model Accuracy', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for i, (key, value) in enumerate(results.items()):\n",
        "    plt.text(i, value + 0.02, f'{value:.4f}', ha='center', fontsize=11)\n",
        "\n",
        "# Training time comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(train_times.keys(), train_times.values(), color=['#3498db', '#2ecc71', '#e74c3c'])\n",
        "plt.title('Training Time', fontsize=14)\n",
        "plt.ylabel('Time (seconds)', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for i, (key, value) in enumerate(train_times.items()):\n",
        "    plt.text(i, value + 0.5, f'{value:.2f}s', ha='center', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = max(results, key=results.get)\n",
        "print(f\"The best performing model is: {best_model_name} with accuracy: {results[best_model_name]:.4f}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ_WXPmMxuKf"
      },
      "source": [
        "## Step 11: Save the Best Student Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1Jb7gj5xuKf"
      },
      "source": [
        "print(\"Saving the best student model...\")\n",
        "best_model = models[best_model_name]\n",
        "joblib.dump(best_model, 'distilled_model.joblib')\n",
        "print(f\"Model saved as 'distilled_model.joblib'\")\n",
        "\n",
        "# You can download this file from Colab to your local machine\n",
        "from google.colab import files\n",
        "files.download('distilled_model.joblib')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iYfL4R_xuKf"
      },
      "source": [
        "## Step 12: Test with Some New Examples\n",
        "\n",
        "Let's see how our best student model performs on some new examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBJgas1NxuKf"
      },
      "source": [
        "print(\"Testing with some examples:\")\n",
        "test_texts = [\n",
        "    \"This movie was amazing and I loved every minute of it!\",\n",
        "    \"The film was terrible and a complete waste of time.\",\n",
        "    \"It had its moments, but overall I found it rather mediocre.\",\n",
        "    \"Probably one of the best films I've seen this year, highly recommend!\",\n",
        "    \"I was expecting more from this movie given all the hype around it.\"\n",
        "]\n",
        "\n",
        "# Process test texts\n",
        "test_embeddings = get_model_embeddings(test_texts, tokenizer, teacher_model)\n",
        "predictions = best_model.predict(test_embeddings)\n",
        "\n",
        "# If the model has predict_proba, get the confidence scores\n",
        "if hasattr(best_model, 'predict_proba'):\n",
        "    probas = best_model.predict_proba(test_embeddings)\n",
        "    has_proba = True\n",
        "else:\n",
        "    has_proba = False\n",
        "\n",
        "for i, (text, pred) in enumerate(zip(test_texts, predictions)):\n",
        "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Prediction: {sentiment}\")\n",
        "\n",
        "    if has_proba:\n",
        "        confidence = probas[i][pred]\n",
        "        print(f\"Confidence: {confidence:.4f}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9PQVnYTxuKf"
      },
      "source": [
        "## Step 13: How to Use the Saved Model in the Future\n",
        "\n",
        "Here's how to use your saved model in the future (this is just for reference, not to run now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxUhO0NUxuKf"
      },
      "source": [
        "# This is a reference for future use - no need to run this now\n",
        "\n",
        "'''\n",
        "import joblib\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "distilled_model = joblib.load('distilled_model.joblib')\n",
        "\n",
        "# Function to get embeddings from your text\n",
        "def get_embedding(text):\n",
        "    # Use the same model as in training\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and get embedding\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                     truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "    return embedding\n",
        "\n",
        "# Make a prediction\n",
        "text = \"Your text here\"\n",
        "embedding = get_embedding(text)\n",
        "prediction = distilled_model.predict(embedding)\n",
        "sentiment = \"Positive\" if prediction[0] == 1 else \"Negative\"\n",
        "print(f\"Sentiment: {sentiment}\")\n",
        "'''"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU9XNWP-xuKf"
      },
      "source": [
        "## Advanced: Using a Quantized Llama Model (Optional)\n",
        "\n",
        "If you want to try using a Llama model, you can use quantization to make it run on Colab's GPUs. Uncomment and run this cell, but note that it might still require a Colab Pro subscription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_15L-eaqxuKf"
      },
      "source": [
        "# # Install additional packages for quantized models\n",
        "# !pip install bitsandbytes accelerate\n",
        "\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# # Use 4-bit quantization for memory efficiency\n",
        "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Or any other Llama model you have access to\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     load_in_4bit=True,\n",
        "#     device_map=\"auto\"\n",
        "# )\n",
        "\n",
        "# # You would need to modify the embedding extraction function to work with this model\n",
        "# # Llama models often need a different approach to extract embeddings"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLw4fHd8xuKf"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've successfully implemented model distillation by:\n",
        "\n",
        "1. Using a pre-trained language model as a teacher\n",
        "2. Extracting embeddings that capture the model's knowledge\n",
        "3. Training smaller, faster student models on these embeddings\n",
        "4. Evaluating and comparing the performance of different student models\n",
        "\n",
        "This approach allows you to benefit from the power of large language models while deploying much smaller and faster models for your specific task."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}